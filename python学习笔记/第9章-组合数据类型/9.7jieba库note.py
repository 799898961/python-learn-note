# 9.7jieba库note.py



# jieba库是优秀的中文分词第三方库
# jieba库分词依靠中文词库，确定汉字之间的关联概率
# 汉字间概率大的组成词组，形成分词结果
# 除了分词，用户还可以添加自定义的词组

# jieba分词的三种模式：
# 精确模式：把文本精确分开，不存在冗余单词
# 全模式：把文本中所有可能的词语都扫描出来。有冗余
# 搜索引擎模式：在精确模式下，对长词再次进行拆分

# 常用函数：
# (1) jieba.lcut(s)    精确模式，返回一个列表类型的分词结果
# 例：
#   jieba.lcut("中国是一个伟大的国家")
#   ——>  ['中国', '是', '一个', '伟大', '的', '国家']
# 
# (2) jieba.lcut(s, cut_all=Ture)
#   全模式，返回一个列表类型的分词结果，存在冗余
# 例：
#   jieba.lcut("中国是一个伟大的国家")
#   ——>  ['中国', '国是', '一个', '伟大', '的', '国家']
#
# (3) jieba.lcut_for_search(s)
#  搜索引擎模式，返回一个列表类型的分词结果，存在冗余
# 例：
#   jieba.lcut_for_search("中华人民共和国是伟大的")
#   ——>  ['中华', '华人', '人民', '共和', '共和国', '中华人民共和国', '是', '伟大', '的']
#
# (4) jieba.add_word(w)    向分词词典增加新词w